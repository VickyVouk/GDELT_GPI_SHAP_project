{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our search space for grid search\n",
    "random_grid = {\n",
    "    'objective' : ['reg:squarederror'],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'max_depth': range(3, 10),\n",
    "    'colsample_bytree': [i/10.0 for i in range(1, 3)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_zero_variables(df):\n",
    "    cols = df.columns\n",
    "    cols_non_0 = []\n",
    "    for col in cols:\n",
    "        if ((df[col].eq(0).sum(axis=0)) < 0.4 * (len(df[col]))): #if we have more than 60% zeros\n",
    "            cols_non_0.append(col)\n",
    "    return cols_non_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = 0.5\n",
    "\n",
    "for i in os.listdir(path + 'all_variables_and_GPI_monthly_all_countries/'):\n",
    "    \n",
    "    if i != '.DS_Store':\n",
    "        \n",
    "        country = i.split('_')[-1].split('.')[0]\n",
    "        print(country)\n",
    "\n",
    "        all_var = pd.read_csv('../../all_variables_and_GPI_monthly_all_countries/'+ i, index_col = 0)\n",
    "\n",
    "        #Delete all columns that have more than 60% of their values 0\n",
    "        variables_non_0 = non_zero_variables(all_var) #Filter the variables that have many zeros\n",
    "        df_country = all_var[variables_non_0]\n",
    "\n",
    "        #Set the target variable\n",
    "        Y = df_country['GPI']\n",
    "\n",
    "        #Set the independent variables\n",
    "        X = df_country.loc[:, df_country.columns != 'GPI']\n",
    "\n",
    "        #Set the training sets:\n",
    "        Y_train = Y[:int(Y.shape[0]*train_set)]\n",
    "        X_train = X[:int(X.shape[0]*train_set)]\n",
    "\n",
    "        #Set the test sets\n",
    "        X_test = X[int(X.shape[0]*train_set):]\n",
    "        Y_test = Y[int(Y.shape[0]*train_set):]\n",
    "\n",
    "        ### DYNAMIC TRAINING:\n",
    "\n",
    "        #Create a dataframe to add the predictions\n",
    "        Predictions = pd.DataFrame(columns = ['MonthYear','prediction1','prediction2','prediction3','prediction4',\n",
    "                                          'prediction5','prediction6','prediction7','prediction8','prediction9',\n",
    "                                          'prediction10','prediction11','prediction12'])\n",
    "\n",
    "        #Create an empty dataframe for the variables\n",
    "        all_importances =  pd.DataFrame(columns = ['Variable', 'Importance','MonthYear'])\n",
    "\n",
    "        k = 0\n",
    "\n",
    "        for i in range(0, len(X_test)): #We have to do 73 trainings\n",
    "\n",
    "            print(i)#print the cycle\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=10).split(X_train)\n",
    "\n",
    "            l_predictions = []\n",
    "\n",
    "           #Train the model\n",
    "\n",
    "            xg_reg = xgb.XGBRegressor() #model to tune\n",
    "\n",
    "\n",
    "            xg_reg_random = GridSearchCV(estimator = xg_reg, param_grid = random_grid,\n",
    "                                            cv = tscv,  n_jobs = -1)\n",
    "\n",
    "            #Best model\n",
    "            model = xg_reg_random.fit(X_train, Y_train)\n",
    "\n",
    "            #If we have less than 12 data points ahead to predict, then increase the value of the k variable\n",
    "            if (len(X_test) - i) < 12: \n",
    "                k = k + 1 \n",
    "                #print('k:',k)\n",
    "\n",
    "            #Make the prediction(s)\n",
    "            for j in range(i, i+12-k):\n",
    "                Y_pred = model.predict(X_test.iloc[[j]])\n",
    "                l_predictions.append(Y_pred[0])\n",
    "\n",
    "            #k variable helps me understand how many months ahead I can predict\n",
    "            if k>0:\n",
    "                l_predictions2 = []\n",
    "                for l in range(0, k):\n",
    "                    l_predictions2.append('-')\n",
    "                #Concatenate the predictions list and the '-' list to add them on the dataframe\n",
    "                l_predictions = l_predictions + l_predictions2\n",
    "\n",
    "            #Concatenate the month and Year of the last training with the prediction list to add them on the dataframe\n",
    "            l_predictions = [str(X_train.index[-1])] + l_predictions\n",
    "\n",
    "            #Add monthyear and predictions on the dataframe\n",
    "            Predictions_length = len(Predictions)\n",
    "            Predictions.loc[Predictions_length] = l_predictions\n",
    "\n",
    "            #Variables importance\n",
    "            importances = model.best_estimator_.feature_importances_ #get the variables importance\n",
    "\n",
    "            #Match the values with the names of the variables\n",
    "            dict_variables = {}\n",
    "            for feat, importance in zip(X_train, importances):\n",
    "                dict_variables[feat] = importance\n",
    "\n",
    "            df_importances = pd.DataFrame(dict_variables.items(), columns=['Variable', 'Importance'])\n",
    "\n",
    "            df_importances['MonthYear'] = str(X_train.index[-1])#get the last month of the training\n",
    "\n",
    "            all_importances = all_importances.append(df_importances).reset_index(drop = True) #add to the total df for importance\n",
    "\n",
    "\n",
    "            #Set the new training sets\n",
    "            X_train = X_train.iloc[1:]\n",
    "            X_train = X_train.append(X_test.iloc[i])\n",
    "            Y_train = Y_train[1:]\n",
    "            Y_train = Y_train.append(pd.Series(Y_test.iloc[i]))\n",
    "            #print(len(X_train))\n",
    "\n",
    "        #Create the list to save the result analytics\n",
    "        Pearson = []\n",
    "        Rmse = []\n",
    "        Mape = []\n",
    "\n",
    "        j = 0\n",
    "        for column in Predictions.iloc[:,1:]:\n",
    "            # Select column contents by column name using [] operator\n",
    "            Pred_col = Predictions[column]\n",
    "            mask = Pred_col.isin(['-'])\n",
    "            Preds = Pred_col.loc[-mask]\n",
    "            test_data = Y_test[j:]\n",
    "            pearson = scipy.stats.pearsonr(test_data, Preds)[0]\n",
    "            Pearson.append(pearson)\n",
    "            rms = sqrt(mean_squared_error(test_data, Preds))\n",
    "            Rmse.append(rms)\n",
    "            mape = mean_absolute_percentage_error(test_data, Preds)\n",
    "            Mape.append(mape)\n",
    "\n",
    "            j = j+1\n",
    "\n",
    "        df_results_analytics = pd.DataFrame(list(zip(Pearson, Rmse, Mape)), \n",
    "                   columns =['Pearson', 'Rmse', 'Mape'])\n",
    "\n",
    "        df_results_analytics.to_csv('../../xgb_results/%s_xgb_results.csv' %country)\n",
    "\n",
    "        #Save the predictions\n",
    "        Predictions.to_csv('../../xgb_results/%s_xgb_predictions.csv' %country)\n",
    "\n",
    "        #Save the importances\n",
    "        all_importances.to_csv('../../xgb_results/%s_xgb_impvar.csv' %country)\n",
    "\n",
    "        #Print the time the iteration ended\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(\"Time ended - country %s =\" %country, current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'YM'\n",
    "df_results_analytics.to_csv('/Users/vickyvoukelatou/Gdelt/GPI_project/journal/results_onlyevents_filt60/%s_xgb_results_short_training_2y.csv' %country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = []\n",
    "for i in allcol:\n",
    "    code = i.split('_')[-1]\n",
    "    if code not in all_codes:\n",
    "        all_codes.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_var = pd.read_csv(path + 'all_variables_j/all_variables_UK.csv', index_col = 0)\n",
    "all_var = all_var[all_var.columns.drop(list(all_var.filter(regex='tone')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gold = all_var[all_var.columns.drop(list(all_var.filter(regex='event')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "for i in all_gold.columns[1:]:\n",
    "    code = i.split('_')[-1]\n",
    "    pr = scipy.stats.pearsonr(all_var['event_count_%s'%code], all_var['goldstein_%s'%code])[0]\n",
    "    my_dict[code]=pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(my_dict.items()),columns = ['code','corr'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
